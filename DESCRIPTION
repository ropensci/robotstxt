Package: robotstxt
Date: 2017-10-17
Type: Package
Title: A 'robots.txt' Parser and 'Webbot'/'Spider'/'Crawler' Permissions Checker
Version: 0.5.0
Authors@R: c(
        person(
          "Peter", "Meissner", role = c("aut", "cre"),
          email = "retep.meissner@gmail.com"
        ),
	      person("Oliver", "Keys", role = "ctb"),
	      person("Rich", "Fitz John", role = "ctb")
	      )
Description: Provides functions to download and parse 'robots.txt' files.
        Ultimately the package makes it easy to check if bots
        (spiders, crawler, scrapers, ...) are allowed to access specific
        resources on a domain.
License: MIT + file LICENSE
LazyData: TRUE
BugReports: https://github.com/ropenscilabs/robotstxt/issues
URL: https://github.com/ropenscilabs/robotstxt
Imports:
    stringr (>= 1.0.0),
    httr (>= 1.0.0),
    spiderbar (>= 0.2.0),
    magrittr
Suggests:
    knitr,
    rmarkdown,
    dplyr,
    testthat,
    covr
Depends:
    R (>= 3.0.0)
VignetteBuilder: knitr
RoxygenNote: 6.0.1
